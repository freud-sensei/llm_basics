{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQFKuX8cmdBvVZPBb608FP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. 트랜스포머 아키텍처란"],"metadata":{"id":"OHUiqwP4iLBm"}},{"cell_type":"markdown","source":["**RNN vs 트랜스포머**\n","* RNN: 텍스트를 순차적으로 하나씩 입력\n","    * 이전 토큰의 출력을 다시 모델에 입력으로 사용 - 순차적 처리\n","    * 느린 학습 속도\n","    * 입력이 길어지면 토큰의 정보가 희석되면서 성능이 떨어짐\n","    * 층을 깊이 쌓으면 gradient vanishing, gradient exploding 발생\n","\n","* 트랜스포머\n","    * **self-attention**: 입력된 문장 내 **각 단어의 관련성을 계산해**, 각 단어의 표현을 조정\n","    * 확장성 용이, 학습 시간 단축, 입력이 길어져도 성능 유지\n","    * 인코더(언어를 이해) + 디코더(언어를 생성) 구조"],"metadata":{"id":"meRsHv9_jIsG"}},{"cell_type":"markdown","source":["# 2. 텍스트를 임베딩으로 변환하기"],"metadata":{"id":"Qz4Tr1Q8ln73"}},{"cell_type":"markdown","source":["텍스트를 모델에 입력할 수 있는 숫자형 데이터인 임베딩으로 변환\n","* **tokenisation**: 텍스트를 적절한 단위로 잘라, 숫자형 id를 부여\n","* **토큰 임베딩 층**: 토큰 id를 여러 숫자의 집합인 토큰 임베딩으로 변환\n","* **위치 인코딩 층**: 토큰의 위치 정보 추가"],"metadata":{"id":"z_JazGCNmDqt"}},{"cell_type":"markdown","source":["## 2.1 tokenisation / 토큰화"],"metadata":{"id":"pu08EJ1Rmqo9"}},{"cell_type":"markdown","source":["* 텍스트를 적절한 단위로 나누고 숫자 ID를 부여\n","* subword 토큰화 방식\n","    * 자주 나오는 단어는 단어 단위 그대로, 가끔 나오는 단어는 작은 단위로 나눔\n","    * 사전의 크기를 작고 효율적으로 유지\n","    "],"metadata":{"id":"8-6gWRb0msBt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FmIAe7Sifw_Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738748591290,"user_tz":-540,"elapsed":759,"user":{"displayName":"­송상록 / 학생 / 심리학과","userId":"18164983390300526999"}},"outputId":"8047b39e-bf6b-405a-8a40-be2d2477ad0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_text_list: ['백두산', '맑은', '정기', '타고', '난', '우리']\n","word2idx: {'백두산': 0, '맑은': 1, '정기': 2, '타고': 3, '난': 4, '우리': 5}\n","idx2word: {0: '백두산', 1: '맑은', 2: '정기', 3: '타고', 4: '난', 5: '우리'}\n","input_ids: [0, 1, 2, 3, 4, 5]\n"]}],"source":["# 단어 단위 토큰화\n","input_text = \"백두산 맑은 정기 타고 난 우리\"\n","input_text_list = input_text.split()\n","print(f\"input_text_list: {input_text_list}\")\n","\n","# 토큰 -> ID 및 ID -> 토큰 딕셔너리 만들기\n","word2idx = {word: idx for idx, word in enumerate(input_text_list)}\n","idx2word = {idx: word for idx, word in enumerate(input_text_list)}\n","print(f\"word2idx: {word2idx}\")\n","print(f\"idx2word: {idx2word}\")\n","\n","# 토큰을 토큰 ID로 변화\n","input_ids = [word2idx[word] for word in input_text_list]\n","print(f\"input_ids: {input_ids}\")"]},{"cell_type":"markdown","source":["## 2.2 token embedding / 토큰 임베딩으로 변환하기\n","* 토큰과 토큰 사이의 관계를 계산할 수 있어야 함\n","* **토큰 임베딩**: 토큰 id를 의미를 담아 숫자 집합으로 변환"],"metadata":{"id":"I3M9CekUonfT"}},{"cell_type":"code","source":["import torch\n","torch.tensor(input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2vuxq9yaqOuX","executionInfo":{"status":"ok","timestamp":1738748592573,"user_tz":-540,"elapsed":16,"user":{"displayName":"­송상록 / 학생 / 심리학과","userId":"18164983390300526999"}},"outputId":"ec07d706-ebe6-4fc2-bf8b-2d1ef9b0ac6a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3, 4, 5])"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","embedding_dim = 16\n","embed_layer = nn.Embedding(len(word2idx), embedding_dim)\n","\n","input_embeddings = embed_layer(torch.tensor(input_ids)) # (6, 16)\n","input_embeddings = input_embeddings.unsqueeze(0) # (1, 6, 16)\n","input_embeddings.shape\n","# 1개의 문장, 5개의 토큰, 16차원의 임베딩"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnX-0exXpWyg","executionInfo":{"status":"ok","timestamp":1738748592573,"user_tz":-540,"elapsed":15,"user":{"displayName":"­송상록 / 학생 / 심리학과","userId":"18164983390300526999"}},"outputId":"af2b653d-97c7-4700-9bb6-e395270c714a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 6, 16])"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["* 현재 임베딩 층은 토큰 아이디를 16차원의 임의의 숫자 집합으로 바꿈\n","* 딥러닝 모델이 학습되는 과정에서, 임베딩 층도 데이터의 의미를 잘 담은 임베딩을 만들도록 학습됨"],"metadata":{"id":"PNXUYotSrejO"}},{"cell_type":"markdown","source":["## 2.3 position encoding / 위치 인코딩"],"metadata":{"id":"0p3o9jn2sIKx"}},{"cell_type":"markdown","source":[],"metadata":{"id":"9CDxFJ1LsWAF"}},{"cell_type":"markdown","source":["* 트랜스포머는 모든 입력을 동시에 처리 - 순서 정보가 사라짐\n","* 순서 정보 처리 역할을 위치 인코딩이 담당\n","* **absolute position encoding**: 토큰 위치에 따라 고정된 임베딩을 더해줌 (e.g., 수식 or 위치에 따른 임베딩 층)\n","* **relative position encoding**: 토큰과 토큰 사이 상대적 위치 정보를 활용해, 긴 텍스트 추론 가능"],"metadata":{"id":"DtJzp1wzsOFk"}},{"cell_type":"code","source":["# 위치 인덱스에 따라 임베딩을 더하도록 구현한 레이어\n","embedding_dim = 16\n","max_position = 12 # 최대 토큰 수\n","\n","embed_layer = nn.Embedding(len(word2idx), embedding_dim)\n","position_embed_layer = nn.Embedding(max_position, embedding_dim)\n","\n","position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0) # (1, 6)\n","position_encodings = position_embed_layer(position_ids) # (1, 6, 16)\n","token_embeddings = embed_layer(torch.tensor(input_ids)) # (6, 16)\n","token_embeddings = token_embeddings.unsqueeze(0) # (1, 6, 16)\n","\n","input_embeddings = token_embeddings + position_encodings\n","input_embeddings.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRzwCwZ-rcu9","executionInfo":{"status":"ok","timestamp":1738748592574,"user_tz":-540,"elapsed":14,"user":{"displayName":"­송상록 / 학생 / 심리학과","userId":"18164983390300526999"}},"outputId":"e4c98b07-7156-4698-fa94-7e97bd726c00"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 6, 16])"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["# 3. 어텐션 이해하기"],"metadata":{"id":"q7fQe2lAwKo1"}},{"cell_type":"markdown","source":["## 3.1 사람이 글을 읽는 방법과 어텐션\n","\n","* 어텐션: 단어와 단어 사이의 관계를 계산\n","* 관련이 깊은 단어와 그렇지 않은 단어 구분\n","* 관련이 깊을수록 더 많이 맥락을 반영해야 함"],"metadata":{"id":"VLv90x3uw91m"}},{"cell_type":"markdown","source":["## 3.2 쿼리, 키, 값 이해하기"],"metadata":{"id":"GvpAFaoSx3xY"}},{"cell_type":"markdown","source":["e.g., '나는 최근 파리 여행을 다녀왔다'\n","\n","* 쿼리: '파리'와 관련 있는 단어를 찾고자 함 -> '파리'\n","* 키: 쿼리와 관련이 있는지 계산하기 위해 문서가 가진 특징 -> 문장 속의 각 단어\n","* 값: 쿼리와 관련 있는 키 -> '여행을', '다녀왔다'\n","\n","'파리'와 키 집합의 관계 개산하기\n","* '파리'의 임베딩과 각 토큰의 임베딩의 관계 계산\n","* $W_Q$, $W_K$ 가중치를 통해 토큰 임베딩 변환\n","* 변환된 토큰 임베딩끼리 벡터곱을 통해 관련도 계산\n","* 관련도와 $W_V$를 통해 변환된 토큰 임베딩 값을 가중합하여 주변 맥락을 반영한 결과를 얻을 수 있음\n"],"metadata":{"id":"YX6zxfnpydg4"}},{"cell_type":"markdown","source":["# 3.3 코드로 보는 어텐션"],"metadata":{"id":"yq6XjZ303DBL"}},{"cell_type":"code","source":["# 쿼리, 키, 값 벡터를 만드는 nn.Linear 층의 출력 차원\n","head_dim = 16\n","\n","# 쿼리, 키, 값을 계산하기 위한 변환\n","weight_q = nn.Linear(embedding_dim, head_dim)\n","weight_k = nn.Linear(embedding_dim, head_dim)\n","weight_v = nn.Linear(embedding_dim, head_dim)\n","\n","querys = weight_q(input_embeddings) # (1, 6, 16)\n","keys = weight_k(input_embeddings) # (1, 6, 16)\n","values = weight_v(input_embeddings) # (1, 6, 16)"],"metadata":{"id":"luks6au4x57g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 스케일 점곱 방식의 어텐션\n","from math import sqrt\n","import torch.nn.functional as F\n","\n","def compute_attention(querys, keys, values, is_causal=False):\n","    dim_k = querys.size(-1) # 16\n","\n","    # 1. 쿼리와 키를 곱한다.\n","    # 분산이 커지는 것을 방지하기 위해 임베딩 차원 수의 제곱으로 나눈다.\n","    scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n","\n","    # 2. 스코어의 합이 1이 되도록 소프트맥스를 취해 가중치로 바꾼다.\n","    weights = F.softmax(scores, dim=-1)\n","\n","    # 3. 가중치와 값을 곱해 입력과 동일한 형태로 출력한다.\n","    return weights @ values"],"metadata":{"id":"pj0t90nZ4c87"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"원본 입력 형태: {input_embeddings.shape}\")\n","after_attention_embeddings = compute_attention(querys, keys, values)\n","print(f\"어텐션 적용 후 형태: {after_attention_embeddings.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lyUlz7r46qrp","executionInfo":{"status":"ok","timestamp":1738748592574,"user_tz":-540,"elapsed":12,"user":{"displayName":"­송상록 / 학생 / 심리학과","userId":"18164983390300526999"}},"outputId":"98a8d2fc-377e-457a-b6cb-da26718409bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["원본 입력 형태: torch.Size([1, 6, 16])\n","어텐션 적용 후 형태: torch.Size([1, 6, 16])\n"]}]},{"cell_type":"code","source":["# 지금까지의 어텐션 과정을 클래스로 나타내기\n","class AttentionHead(nn.Module):\n","    def __init__(self, token_embed_dim, head_dim, is_causal=False):\n","        super().__init__()\n","        self.is_causal = is_causal\n","        self.weight_q = nn.Linear(token_embed_dim, head_dim)\n","        self.weight_k = nn.Linear(token_embed_dim, head_dim)\n","        self.weight_v = nn.Linear(token_embed_dim, head_dim)\n","\n","    def forward(self, querys, keys, values):\n","        outputs = compute_attention(\n","            self.weight_q(querys),\n","            self.weight_k(keys),\n","            self.weight_v(values),\n","            is_causal=self.is_causal\n","        )\n","        return outputs\n","\n","attention_head = AttentionHead(embedding_dim, embedding_dim)\n","after_attention_embeddings = attention_head(input_embeddings, input_embeddings, input_embeddings)"],"metadata":{"id":"l_qOeCKL78r3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.4 멀티헤드 어텐션\n","* 여러 어텐션 연산을 동시에 적용하기\n","* 여러 측면을 동시에 고려하기 위한 절차\n","* 헤드의 수만큼 연산을 수행하고, 각각의 어텐션을 계산한 뒤, 입력과 같은 형태로 반환하고, 선형 층을 통과시켜 최종 결과 반환"],"metadata":{"id":"icuYQsMOBCQ_"}},{"cell_type":"code","source":["# 지금까지의 어텐션 과정을 클래스로 나타내기\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, token_embed_dim, d_model, n_head, is_causal=False):\n","        super().__init__()\n","        self.n_head = n_head\n","        self.is_causal = is_causal\n","        self.weight_q = nn.Linear(token_embed_dim, d_model)\n","        self.weight_k = nn.Linear(token_embed_dim, d_model)\n","        self.weight_v = nn.Linear(token_embed_dim, d_model)\n","        self.concat_linear = nn.Linear(d_model, d_model)\n","\n","    def forward(self, querys, keys, values):\n","        B, T, C = querys.size()\n","\n","        # Q, K, V가 처음 통과하는 선형 층\n","        # 쿼리, 키, 값을 n_head개로 쪼개고 각각의 어텐션을 계산\n","        querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","        keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","        values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","        print(querys.shape)\n","\n","        # h(헤드 수)번의 스케일 점곱 어텐션\n","        attention = compute_attention(querys, keys, values, self.is_causal)\n","        print(attention.shape)\n","\n","        # 어텐션 결과의 연결\n","        output = attention.transpose(1, 2).contiguous().view(B, T, C)\n","\n","        # 마지막 선형 층\n","        output = self.concat_linear(output)\n","        return output\n","\n","n_head = 4\n","mh_attention = MultiHeadAttention(embedding_dim, embedding_dim, n_head)\n","after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n","after_attention_embeddings.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7Wvg7mpBg1w","executionInfo":{"status":"ok","timestamp":1738748592574,"user_tz":-540,"elapsed":10,"user":{"displayName":"­송상록 / 학생 / 심리학과","userId":"18164983390300526999"}},"outputId":"c08c7ea7-30b8-4120-8678-c86111d4a3dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 4, 6, 4])\n","torch.Size([1, 4, 6, 4])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 6, 16])"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["# 4. 정규화와 피드 포워드 층"],"metadata":{"id":"0X2Zx1-qGqNl"}},{"cell_type":"markdown","source":["## 4.1 층 정규화 (layer normalisation)\n","\n","* 데이터 정규화를 통해 모든 입력 변수가 비슷한 범위와 분포를 갖도록 조정할 수 있음\n","* $\\textbf{norm_x} = (\\textbf{x} - 평균) / 표준편차$\n","\n","* 배치 정규화: 모델에 입력으로 들어가는 미니 배치 사이에 정규화 수행\n","    * 자연어 처리에서는 입력 문장의 길이가 다양\n","    * 정규화에 포함되는 데이터의 수가 제각각이라 효과 낮음 (패딩 토큰 존재)\n","\n","* 층 정규화: 각 샘플(토큰 임베딩) 내부의 평균과 표준편차를 구해 정규화를 수행\n","    * 각각 샘플별로 정규화를 수행하기 때문에, 정규화 효과에 차이 없음\n","\n","* 사후 정규화에 비해, 어텐션 및 피드 포워드 이전에 정규화를 하는 사전 정규화가 주로 활용됨"],"metadata":{"id":"y6CgrHMHHE2V"}},{"cell_type":"code","source":["norm = nn.LayerNorm(embedding_dim)\n","norm_x = norm(input_embeddings)\n","print(norm_x.shape) # 1, 6, 16\n","\n","print(norm_x.mean(axis=-1).data, norm_x.std(axis=-1).data)\n","# 스케일링이 이루어져 완전히 0과 1이랑 동일하지 않을 수 있음."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnOkndMbJScr","executionInfo":{"status":"ok","timestamp":1738748592574,"user_tz":-540,"elapsed":8,"user":{"displayName":"­송상록 / 학생 / 심리학과","userId":"18164983390300526999"}},"outputId":"eae08a72-3771-4590-b354-967025839937"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 6, 16])\n","tensor([[ 2.3283e-08,  2.2352e-08,  4.4703e-08, -7.4506e-09,  3.7253e-08,\n","          2.9802e-08]]) tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328, 1.0328]])\n"]}]},{"cell_type":"markdown","source":["## 4.2 피드 포워드 층 (feed forward layer)\n","\n","* 데이터의 특징을 학습하는 fully connected layer\n","* MHA가 단어 사이 관계를 파악한 역할이라면, 피드 포워드는 입력 텍스트 전체를 이해하는 역할\n","* 층을 쌓고 확장하기 위해 입력층, 출력층의 차원은 동일하게 설정"],"metadata":{"id":"T6WusTv0HIbj"}},{"cell_type":"code","source":["class PreLayerNormFeedForward(nn.Module):\n","    def __init__(self, d_model, dim_feedforward, dropout):\n","        super().__init__()\n","        # 입출력 차원을 동일하게\n","        self.linear1 = nn.Linear(d_model, dim_feedforward)\n","        self.linear2 = nn.Linear(dim_feedforward, d_model)\n","        self.dropout1 = nn.Dropout(dropout) # 드롭아웃 층\n","        self.dropout2 = nn.Dropout(dropout) # 드롭아웃 층\n","        self.activation = nn.GELU() # 활성함수\n","        self.norm = nn.LayerNorm(d_model) # 층 정규화\n","\n","    def forward(self, src):\n","        x = self.norm(src) # 사전 정규화\n","        x = x + self.linear2(self.dropout1(self.activation(self.linear1(x)))) # 잔차 연결-다음절 참고\n","        x = self.dropout2(x)\n","        return x\n"],"metadata":{"id":"YTR8VfgCLLw1","executionInfo":{"status":"ok","timestamp":1738749214697,"user_tz":-540,"elapsed":525,"user":{"displayName":"­송상록 / 학생 / 심리학과","userId":"18164983390300526999"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["# 5. 인코더\n","\n","* layer normalisation -> multi-head attention -> layer normalisation -> feed-forward network 의 반복 형태\n","* 잔차 연결: 출력값에 입력을 다시 더해주는 형태, 안정적 학습에 도움을 줌"],"metadata":{"id":"LdOCX2Dbfx02"}},{"cell_type":"code","source":["class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, d_model, nhead, dim_feedforward, dropout):\n","        super().__init()\n","        self.attn = MultiHeadAttention(d_model, d_model, nhead) # 멀티헤드 어텐션 클래스\n","        self.norm1 = nn.LayerNorm(d_model) # 층 정규화\n","        self.dropout1 = nn.Dropout(dropout) # 드롭아웃\n","        self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout) # 피드 포워드\n","\n","    def forward(self, src):\n","        norm_x = self.norm1(src)\n","        attn_output = self.attn(norm_x, norm_x, norm_x)\n","        x = src + self.dropout1(attn_output)\n","        x = self.feed_forward(x)\n","        return x"],"metadata":{"id":"pTMmCDbygC_3","executionInfo":{"status":"ok","timestamp":1738749471069,"user_tz":-540,"elapsed":348,"user":{"displayName":"­송상록 / 학생 / 심리학과","userId":"18164983390300526999"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["# 인코더 층을 반복하여 쌓기\n","\n","import copy\n","\n","def get_clones(module, N):\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, encoder_layer, num_layers):\n","        super().__init__()\n","        self.layers = get_clones(encoder_layer, num_layers)\n","        self.num_layers = num_layers\n","        self.norm = norm\n","\n","    def forward(self, src):\n","        output = src\n","        for mod in self.layers:\n","            output = mod(output)\n","        return output"],"metadata":{"id":"zfltQcflhbKD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. 디코더\n","* 인코더와 다르게 **마스크 멀티헤드 어텐션** 사용\n","* 생성을 담당: 앞에서 생성한 토큰을 기반으로 다음 토큰 생성 (인과적, causal)\n","* 실제 텍스트를 생성할 때 디코더는 이전까지 생성한 텍스트만 확인할 수 있음\n","* 단, 학습할 때는 인코더와 디코더 모두 완성된 테스트를 입력으로 받음\n","* 미래 시점에 작성해야 하는 텍스트를 미리 확인하게 되는 문제를 막기 위해, 특정 시점엔 그 이전에 생성된 토큰까지만 확인할 수 있게 함 -> **마스킹**"],"metadata":{"id":"ccRFFktZjf-P"}},{"cell_type":"code","source":["# 디코더에서 어텐션 연산 - 마스크 어텐션\n","\n","def compute_attention(querys, keys, values, is_causal=False):\n","    dim_k = querys.size(-1) # 16\n","    scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n","\n","    if is_causal:\n","        query_length = querys.size(-2)\n","        key_length = keys.size(-2)\n","\n","        # 대각선 아래는 True, 위는 False\n","        temp_mask = torch.ones(query_length, key_length, dtype=torch.bool).tril(diagonal=0)\n","\n","        # False인 위치는 -inf로 채움\n","        scores = scores.masked_fill(temp_mask == False, float(\"-inf\"))\n","\n","    # softmax를 취하면 -inf였던 원소는 0이 됨\n","    weights = F.softmax(scores, dim=-1)\n","\n","    return weights @ values"],"metadata":{"id":"guLDg_xIkaiK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* **크로스 어텐션**: 인코더의 결과를 디코더가 활용하는 연산\n","* 영->한 번역 시, 인코더는 영어 문장 입력으로 받고, 처리한 결과를 번역한 한국어를 생성하는 디코더가 받아 활용\n","    * 키, 값: 인코더의 결과\n","    * 쿼리: 디코더의 잠재 상태"],"metadata":{"id":"MQNkZuFQmMta"}},{"cell_type":"code","source":["class TransformerDecoderLayer(nn.Module):\n","    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n","        super().__init()\n","        self.attn1 = MultiHeadAttention(d_model, d_model, nhead) # 멀티헤드 어텐션 클래스\n","        self.attn2 = MultiHeadAttention(d_model, d_model, nhead) # 크로스 어텐션 구현\n","        self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout) # 피드 포워드\n","\n","        self.norm1 = nn.LayerNorm(d_model) # 층 정규화\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(dropout) # 드롭아웃\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","    def forward(self, tgt, encoder_output, is_causal=True):\n","        # 셀프 어텐션\n","        x = self.norm1(tgt)\n","        x = x + self.dropout1(self.attn1(x, x, x, is_causal=is_causal))\n","\n","        # 크로스 어텐션\n","        x = self.norm2(x)\n","        x = x + self.dropout2(self.attn2(x, encoder_output, encoder_output))\n","\n","        # 피드 포워드 연산\n","        x = self.feed_forward(x)\n","        return x"],"metadata":{"id":"syE4K5mDmkzx","executionInfo":{"status":"ok","timestamp":1738750940515,"user_tz":-540,"elapsed":320,"user":{"displayName":"­송상록 / 학생 / 심리학과","userId":"18164983390300526999"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# 디코더 구현\n","import copy\n","def get_clones(module, N):\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","class TransformerDecoder(nn.Module):\n","    def __init__(self, decoder_layer, num_layers):\n","        super().__init__()\n","        self.layers = get_clones(decoder_layer, num_layers)\n","        self.num_layers = num_layers\n","\n","    def forward(self, tgt, src):\n","        output = tgt\n","        for mod in self.layers:\n","            output = mod(tgt, src)\n","        return output"],"metadata":{"id":"AOOulGRiohRe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"EpS7ocwGofrY"}}]}